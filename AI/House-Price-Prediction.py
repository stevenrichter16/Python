# -*- coding: utf-8 -*-
"""Housing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TJs3LHMyjGJta6Nm464WjWDX3Kv4ZFz8
"""

#common imports
import pandas as pd
import sklearn
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import sys
import os

"""# Get Data

* Load the data
"""

#Path to data file
HOUSING_PATH = ""

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

"""* Examine the data"""

housing_full = load_housing_data()
housing_full.head()

housing_full.info()

housing_full["ocean_proximity"].value_counts()

housing_full.describe()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
housing_full.hist(bins=50, figsize=(20,15))
plt.show()

"""**Test set:** Now you keep aside a test set, using purely random sampling. Admittedly, this is not the best approach. In practice, we would want to use stratified sampling to ensure our test set has a representative sample of the population. For now, we won't worry about this"""

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing_full, test_size=0.2, random_state=42)
print(len(housing_full),len(train_set),len(test_set))

"""# Discover and visualize the data to gain insights

Make a copy of the Data
"""

housing = train_set.copy()

"""Plot a geographical scatter plot of the data"""

housing.plot(kind="scatter", x="longitude", y="latitude")

"""Use the alpha values to generate a better visualization with high-density areas highlighted"""

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)

"""California Housing prices with red being expensive, blue being cheap. Larger circles indicate areas with larger populations"""

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
    s=housing["population"]/100, label="population", figsize=(10,7),
    c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
    sharex=False)

"""Since data is not too large, you can try computing the standard correlation coefficient between every pair of attributes. Correlation coefficient varies from 1 (strong positive correlation) to -1(strong negative correlation)"""

corr_matrix = housing.corr()

corr_matrix["median_house_value"].sort_values(ascending=False)

"""# Prepare data

First remove the y column (target attribute) from the data
"""

housing = train_set.drop("median_house_value", axis=1) # drop labels for training set, creates a copy of the data set
housing_labels = train_set["median_house_value"].copy()

"""Total bedrooms has some missing values. We can either drop those rows, with missing values, get rid of the column, or fill the missing data with some canonical values, such as the median value. You will need to use the same process for the test set you have put away!"""

median = housing["total_bedrooms"].median() #need this to fill out any missing values in test set later
housing["total_bedrooms"].fillna(median, inplace=True) # option 3

"""Alternatively, we could use an imputer; this will remember the statistics for us. We will use this approach in the pipeline below.

So let's build a pipeline for preprocessing the numerical attributes, add median imputation and standard scaling
"""

housing_num = housing.drop("ocean_proximity", axis=1)# must drop this non-numeric attribute before next steps

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)

"""For regression based algorithms, categorical attributes are problematic. We can map a category attribute to numbers, but a regressor might misinterpret numbers as distances between options. A better approach is to hot encode the categorical attribute. We combine the hot encoder along with the previous pipeline together. """

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
        ("num", num_pipeline, num_attribs),
        ("cat", OneHotEncoder(sparse=False), cat_attribs),
    ])

housing_prepared = full_pipeline.fit_transform(housing)

housing_prepared

housing_prepared.shape

"""# Select and train a model

Let's first try a Linear Regressor
"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

# let's try the full preprocessing pipeline on a few training instances
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)

print("Predictions:", lin_reg.predict(some_data_prepared))

print("Labels:", list(some_labels))

from sklearn.metrics import mean_squared_error

housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

"""This implies we're more than 68K off the price of a house on average using RMSE. Maybe our model is underfitting the data. 
Let's try this for the Decision Tree version of a regressor. It uses standard deviation reduction instead of Information Gain in the ID3 algorithm
"""

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)

housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

"""Now we have learnt the data too well!

Use cross validation to get a better picture of performance
"""

def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())
    
from sklearn.model_selection import cross_val_score
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)
tree_rmse_scores = np.sqrt(-scores)
display_scores(tree_rmse_scores)

from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg.fit(housing_prepared, housing_labels)

"""BTW, if you want to save a model, you can use the joblib library"""

import joblib
#save a model
joblib.dump(forest_reg, "forest_reg.pkl") 

#load a saved model
forest_reg = joblib.load("forest_reg.pkl")

housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
forest_rmse

forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)

"""# Fine Tune the model"""

from sklearn.model_selection import GridSearchCV

param_grid = [
    # try 12 (3×4) combinations of hyperparameters
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    # then try 6 (2×3) combinations with bootstrap set as False
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

forest_reg = RandomForestRegressor(random_state=42)
# train across 5 folds, that's a total of (12+6)*5=90 rounds of training 
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)
grid_search.fit(housing_prepared, housing_labels)

"""The best feature combination found:"""

grid_search.best_params_

grid_search.best_estimator_

"""Let's look at the score of each hyperparameter combination tested during the grid search:"""

cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

"""Another way to search the hyperparameter space, when it is very large is to use a Randomized Search, instead of a Grid Search. 
We can also combine the best models into a combined ensemble model.

# Run on test set
It seems that Random Forests are performing best so let's test this on our test set.
"""

final_model = grid_search.best_estimator_

X_test = test_set.drop("median_house_value", axis=1)
y_test = test_set["median_house_value"].copy()

# call transform, not fit_transform - you do not want to fit the test set by incorporating its values into the median, etc!
X_test_prepared = full_pipeline.transform(X_test) 
final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)

final_rmse

joblib.dump(final_model, "final_model.pkl")

